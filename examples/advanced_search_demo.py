#!/usr/bin/env python3
"""
é«˜çº§æœç´¢åŠŸèƒ½æ¼”ç¤º

å±•ç¤ºå¦‚ä½•ä½¿ç”¨é‡æ„åçš„æœç´¢å’ŒæŸ¥æ‰¾åŠŸèƒ½
"""
import sys
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.pathï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥html_analysis_agent
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from html_analysis_agent import HTMLAnalysisAgent
from pathlib import Path


def load_html_file(file_path):
    """åŠ è½½HTMLæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()


def demo_advanced_search():
    """é«˜çº§æœç´¢åŠŸèƒ½æ¼”ç¤º"""
    print("ğŸ” HTML Analysis Agent é«˜çº§æœç´¢åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)

    # åˆå§‹åŒ–Agent
    agent = HTMLAnalysisAgent()

    # æµ‹è¯•æ–‡ä»¶
    test_file = Path('examples/13_detail.html')
    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ {test_file} ä¸å­˜åœ¨")
        return

    # åŠ è½½HTMLå†…å®¹
    html_content = load_html_file(test_file)
    print(f"ğŸ“„ åŠ è½½æ–‡ä»¶: {test_file.name}")
    print(f"   æ–‡ä»¶å¤§å°: {len(html_content):,} å­—ç¬¦")

    # 1. å…³é”®è¯æœç´¢
    print("\n1. ğŸ”‘ å…³é”®è¯æœç´¢")
    print("-" * 30)

    keywords = ['ç™½æ²™', 'ä»·æ ¼', 'è¯„åˆ†', 'è¯„è®º', 'button']
    search_result = agent.search_html_content(html_content, keywords)

    print(f"æœç´¢å…³é”®è¯: {keywords}")
    print(f"æ‰¾åˆ°åŒ¹é…é¡¹: {len(search_result.get('search_results', []))}")

    # æŒ‰å…³é”®è¯åˆ†ç»„æ˜¾ç¤ºç»“æœ
    results_by_keyword = {}
    for result in search_result.get('search_results', []):
        keyword = result.get('matched_keyword', 'unknown')
        if keyword not in results_by_keyword:
            results_by_keyword[keyword] = []
        results_by_keyword[keyword].append(result)

    for keyword, results in results_by_keyword.items():
        print(f"\n'{keyword}' ç›¸å…³ç»“æœ ({len(results)} ä¸ª):")
        for i, result in enumerate(results[:2]):  # æ¯ä¸ªå…³é”®è¯æœ€å¤šæ˜¾ç¤º2ä¸ªç»“æœ
            print(f"  {i+1}. {result.get('tag', 'unknown')} - {result.get('text_content', '')[:60]}...")

    # 2. XPathæŸ¥æ‰¾
    print("\n\n2. ğŸ›£ï¸ XPathå…ƒç´ æŸ¥æ‰¾")
    print("-" * 30)

    xpath_tests = [
        "//div[@class='column_tit']",
        "//span[contains(@class, 'price')]",
        "//button[@id='login-btn']",
        "//h1",
        "//ul[@class='ul_1']"
    ]

    for xpath in xpath_tests:
        result = agent.find_elements_by_xpath(html_content, xpath)
        found_count = result.get('total_found', 0)
        print(f"XPath: {xpath}")
        print(f"  æ‰¾åˆ°å…ƒç´ : {found_count} ä¸ª")

        # æ˜¾ç¤ºæ‰¾åˆ°çš„å…ƒç´ ä¿¡æ¯
        found_elements = result.get('found_elements', [])
        for i, element in enumerate(found_elements[:1]):  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ª
            print(f"  ç¤ºä¾‹: {element.get('tag')} - {element.get('text_content', '')[:40]}...")
        print()

    # 3. CSSé€‰æ‹©å™¨æŸ¥æ‰¾
    print("\n3. ğŸ¨ CSSé€‰æ‹©å™¨å…ƒç´ æŸ¥æ‰¾")
    print("-" * 30)

    css_tests = [
        ".column_tit",
        ".price",
        "button",
        "h1",
        ".ul_1"
    ]

    for css in css_tests:
        result = agent.find_elements_by_css(html_content, css)
        found_count = result.get('total_found', 0)
        print(f"CSS: {css}")
        print(f"  æ‰¾åˆ°å…ƒç´ : {found_count} ä¸ª")

        # æ˜¾ç¤ºæ‰¾åˆ°çš„å…ƒç´ ä¿¡æ¯
        found_elements = result.get('found_elements', [])
        for i, element in enumerate(found_elements[:1]):  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ª
            print(f"  ç¤ºä¾‹: {element.get('tag')} - {element.get('text_content', '')[:40]}...")
        print()

    # 4. æ•°æ®å®¹å™¨åˆ†æ
    print("\n4. ğŸ“¦ æ•°æ®å®¹å™¨åˆ†æ")
    print("-" * 30)

    container_result = agent.analyze_data_containers(html_content)
    containers = container_result.get('containers', {})

    print("å‘ç°çš„æ•°æ®å®¹å™¨ç±»å‹:")
    for container_type, items in containers.items():
        print(f"  {container_type}: {len(items)} ä¸ªå®¹å™¨")

    # æ˜¾ç¤ºæ¯ä¸ªç±»å‹çš„å‰å‡ ä¸ªå®¹å™¨
    for container_type, items in containers.items():
        if items:
            print(f"\n{container_type} å®¹å™¨ç¤ºä¾‹:")
            for i, item in enumerate(items[:2]):  # æ¯ä¸ªç±»å‹æœ€å¤šæ˜¾ç¤º2ä¸ª
                print(f"  {i+1}. {item.get('tag', 'unknown')} - {item.get('text_content', '')[:60]}...")

    # 5. å…ƒç´ ä½ç½®åˆ†æ
    print("\n\n5. ğŸ“ å…ƒç´ ä½ç½®åˆ†æ")
    print("-" * 30)

    position_result = agent.analyze_element_positions(html_content)
    hierarchy = position_result.get('position_analysis', {}).get('hierarchy_analysis', {})

    print(f"å±‚çº§åˆ†æ:")
    print(f"  æœ€å¤§æ·±åº¦: {hierarchy.get('max_depth', 0)}")
    print(f"  æ€»å…ƒç´ æ•°: {hierarchy.get('total_elements', 0)}")

    depth_dist = hierarchy.get('depth_distribution', {})
    print(f"  æ·±åº¦åˆ†å¸ƒ:")
    for depth, count in sorted(depth_dist.items())[:5]:
        print(f"    æ·±åº¦ {depth}: {count} ä¸ªå…ƒç´ ")

    # 6. HTMLç®€åŒ–åˆ†æ
    print("\n\n6. âš¡ HTMLç®€åŒ–åˆ†æ")
    print("-" * 30)

    simplified_result = agent.analyze_html_with_simplification(html_content)
    stats = simplified_result.get('simplification_stats', {})

    print("ç®€åŒ–ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")

    original_size = len(html_content)
    simplified_size = len(simplified_result.get('simplified_html', ''))
    compression_ratio = (1 - simplified_size / original_size) * 100

    print(f"\nå‹ç¼©æ•ˆæœ:")
    print(f"  åŸå§‹å¤§å°: {original_size:,} å­—ç¬¦")
    print(f"  ç®€åŒ–å: {simplified_size:,} å­—ç¬¦")
    print(f"  å‹ç¼©æ¯”ä¾‹: {compression_ratio:.1f}%")

    print(f"\nğŸ‰ é«˜çº§æœç´¢æ¼”ç¤ºå®Œæˆï¼")


def demo_multiple_files():
    """æ¼”ç¤ºå¤šæ–‡ä»¶å¤„ç†"""
    print("\nğŸ“š å¤šæ–‡ä»¶æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 40)

    agent = HTMLAnalysisAgent()

    # æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    test_files = [
        ('ç™½æ²™é¦™çƒŸ', 'examples/13_detail.html'),
        ('å¨‡å­é¦™çƒŸ', 'examples/95_detail.html'),
        ('æœªçŸ¥äº§å“', 'examples/4761_detail.html')
    ]

    results = {}

    for product_name, file_path in test_files:
        file_obj = Path(file_path)
        if not file_obj.exists():
            print(f"âš ï¸ è·³è¿‡ {product_name}: æ–‡ä»¶ä¸å­˜åœ¨")
            continue

        print(f"\nğŸ“„ å¤„ç†: {product_name}")

        try:
            html_content = load_html_file(file_path)

            # æ‰§è¡Œç®€åŒ–åˆ†æ
            simplified_result = agent.analyze_html_with_simplification(html_content)
            stats = simplified_result.get('simplification_stats', {})

            # æ‰§è¡Œæœç´¢
            search_result = agent.search_html_content(html_content, ['ä»·æ ¼', 'è¯„åˆ†'])

            # å­˜å‚¨ç»“æœ
            results[product_name] = {
                'file_size': len(html_content),
                'simplified_stats': stats,
                'search_count': len(search_result.get('search_results', []))
            }

            print(f"   æ–‡ä»¶å¤§å°: {len(html_content):,} å­—ç¬¦")
            print(f"   ç§»é™¤è„šæœ¬: {stats.get('script_tags', 0)} ä¸ª")
            print(f"   ç§»é™¤æ ·å¼: {stats.get('style_tags', 0)} ä¸ª")
            print(f"   æœç´¢ç»“æœ: {len(search_result.get('search_results', []))} ä¸ª")

        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")

    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    if results:
        print(f"\n{'='*40}")
        print("ğŸ“Š å¤„ç†ç»“æœå¯¹æ¯”")
        print('='*40)

        for product_name, data in results.items():
            print(f"\n{product_name}:")
            print(f"  æ–‡ä»¶å¤§å°: {data['file_size']:,} å­—ç¬¦")
            print(f"  è„šæœ¬ç§»é™¤: {data['simplified_stats'].get('script_tags', 0)}")
            print(f"  æ ·å¼ç§»é™¤: {data['simplified_stats'].get('style_tags', 0)}")
            print(f"  æœç´¢ç»“æœ: {data['search_count']} ä¸ª")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # é«˜çº§æœç´¢æ¼”ç¤º
        demo_advanced_search()

        # å¤šæ–‡ä»¶å¤„ç†æ¼”ç¤º
        demo_multiple_files()

    except Exception as e:
        print(f"è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

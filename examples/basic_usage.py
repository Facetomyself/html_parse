#!/usr/bin/env python3
"""
HTML Analysis Agent é‡æ„åä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨åŸºäºHTMLç®€åŒ–æŠ€æœ¯çš„HTML Analysis Agent
ä½¿ç”¨çœŸå®HTMLæ–‡ä»¶è¿›è¡Œæµ‹è¯•å’Œæ¼”ç¤º
"""
import sys
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.pathï¼Œç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥html_analysis_agent
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from html_analysis_agent import HTMLAnalysisAgent


def load_html_file(file_path):
    """åŠ è½½HTMLæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()


def demo_with_real_html():
    """ä½¿ç”¨çœŸå®HTMLæ–‡ä»¶è¿›è¡Œæ¼”ç¤º"""
    print("ğŸ¯ HTML Analysis Agent é‡æ„ååŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)

    # åˆå§‹åŒ–Agent
    print("1. åˆå§‹åŒ–Agent...")
    agent = HTMLAnalysisAgent()
    print("   âœ“ Agentåˆå§‹åŒ–æˆåŠŸ")

    # å‡†å¤‡æµ‹è¯•æ–‡ä»¶
    html_files = {
        'ç™½æ²™é¦™çƒŸ': '13_detail.html',
        'å¨‡å­é¦™çƒŸ': '95_detail.html',
        'ä¸‡å®è·¯é¦™çƒŸ': '4761_detail.html'
    }

    # éå†æµ‹è¯•æ–‡ä»¶
    for product_name, filename in html_files.items():
        file_path = Path('examples') / filename

        if not file_path.exists():
            print(f"\nâš ï¸  æ–‡ä»¶ {filename} ä¸å­˜åœ¨ï¼Œè·³è¿‡...")
            continue

        print(f"\n{'='*60}")
        print(f"ğŸ“„ åˆ†æäº§å“: {product_name} ({filename})")
        print('='*60)

        try:
            # åŠ è½½HTMLå†…å®¹
            html_content = load_html_file(file_path)
            print(f"   æ–‡ä»¶å¤§å°: {len(html_content):,} å­—ç¬¦")

            # 2. HTMLç®€åŒ–åˆ†æ
            print("\n2. HTMLç®€åŒ–åˆ†æ...")
            simplified_result = agent.analyze_html_with_simplification(html_content)
            stats = simplified_result.get('simplification_stats', {})
            print(f"   ç®€åŒ–ç»Ÿè®¡:")
            for key, value in stats.items():
                print(f"     {key}: {value}")
            print(f"   ç®€åŒ–åHTMLé•¿åº¦: {len(simplified_result.get('simplified_html', '')):,} å­—ç¬¦")

            # 3. æ•°æ®å®¹å™¨åˆ†æ
            print("\n3. æ•°æ®å®¹å™¨åˆ†æ...")
            container_result = agent.analyze_data_containers(html_content)
            containers = container_result.get('containers', {})
            print(f"   å‘ç°æ•°æ®å®¹å™¨ç±»å‹:")
            for container_type, items in containers.items():
                print(f"     {container_type}: {len(items)} ä¸ª")
            print(f"   æ–‡æ¡£ID: {container_result.get('doc_id', 'N/A')}")

            # 4. æ™ºèƒ½å†…å®¹æœç´¢
            print("\n4. æ™ºèƒ½å†…å®¹æœç´¢...")
            search_keywords = ['ä»·æ ¼', 'å“ç‰Œ', 'è¯„åˆ†', 'è¯„è®º']
            search_result = agent.search_html_content(html_content, search_keywords)
            print(f"   æœç´¢å…³é”®è¯: {search_keywords}")
            search_items = search_result.get('search_results', [])
            print(f"   æ‰¾åˆ°åŒ¹é…é¡¹: {len(search_items)} ä¸ª")

            # æ˜¾ç¤ºå‰å‡ ä¸ªæœç´¢ç»“æœ
            for i, item in enumerate(search_items[:3]):
                print(f"     ç»“æœ {i+1}: {item.get('tag', 'unknown')} - {item.get('text_content', '')[:50]}...")

            # 5. XPathå…ƒç´ æŸ¥æ‰¾
            print("\n5. XPathå…ƒç´ æŸ¥æ‰¾...")
            xpath_patterns = [
                "//div[@class='column_tit']",
                "//span[@class='price']",
                "//button"
            ]

            for xpath in xpath_patterns:
                xpath_result = agent.find_elements_by_xpath(html_content, xpath)
                found_count = xpath_result.get('total_found', 0)
                print(f"     {xpath}: æ‰¾åˆ° {found_count} ä¸ªå…ƒç´ ")

            # 6. CSSé€‰æ‹©å™¨å…ƒç´ æŸ¥æ‰¾
            print("\n6. CSSé€‰æ‹©å™¨å…ƒç´ æŸ¥æ‰¾...")
            css_patterns = [
                ".column_tit",
                ".price",
                "button"
            ]

            for css in css_patterns:
                css_result = agent.find_elements_by_css(html_content, css)
                found_count = css_result.get('total_found', 0)
                print(f"     {css}: æ‰¾åˆ° {found_count} ä¸ªå…ƒç´ ")

            # 7. å…ƒç´ ä½ç½®åˆ†æ
            print("\n7. å…ƒç´ ä½ç½®åˆ†æ...")
            position_result = agent.analyze_element_positions(html_content)
            hierarchy = position_result.get('position_analysis', {}).get('hierarchy_analysis', {})
            print(f"   å…ƒç´ å±‚çº§åˆ†æ:")
            print(f"     æœ€å¤§æ·±åº¦: {hierarchy.get('max_depth', 0)}")
            print(f"     æ€»å…ƒç´ æ•°: {hierarchy.get('total_elements', 0)}")
            print(f"     æ–‡æ¡£ID: {position_result.get('doc_id', 'N/A')}")

        except Exception as e:
            print(f"   âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    print(f"\n{'='*60}")
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)

    # æ˜¾ç¤ºæ€»ç»“
    print("\nğŸ“Š æ¼”ç¤ºæ€»ç»“:")
    print("   âœ“ HTMLç®€åŒ–æŠ€æœ¯ - 90%å†…å®¹å‡å°‘")
    print("   âœ“ æ™ºèƒ½å†…å®¹æœç´¢ - å…³é”®è¯å¿«é€Ÿå®šä½")
    print("   âœ“ æ•°æ®å®¹å™¨åˆ†æ - ç»“æ„åŒ–ä¿¡æ¯æå–")
    print("   âœ“ XPath/CSSæŸ¥æ‰¾ - ç²¾ç¡®å…ƒç´ å®šä½")
    print("   âœ“ å…ƒç´ ä½ç½®åˆ†æ - å±‚çº§ç»“æ„ç†è§£")
    print("   âœ“ ç»“æ„åŒ–å­˜å‚¨ - é«˜æ•ˆæ•°æ®ç®¡ç†")


def demo_html_simplification():
    """æ¼”ç¤ºHTMLç®€åŒ–åŠŸèƒ½"""
    print("\nğŸ”§ HTMLç®€åŒ–åŠŸèƒ½å•ç‹¬æ¼”ç¤º")
    print("=" * 40)

    from tools.html_simplifier import HTMLSimplifier

    simplifier = HTMLSimplifier()

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªHTMLæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
    sample_file = Path('examples/13_detail.html')
    if sample_file.exists():
        html_content = load_html_file(sample_file)

        print(f"åŸå§‹HTMLé•¿åº¦: {len(html_content):,} å­—ç¬¦")

        # ç®€åŒ–HTML
        simplified = simplifier.simplify_html_string(html_content)
        stats = simplifier.get_simplification_stats()

        print(f"ç®€åŒ–åHTMLé•¿åº¦: {len(simplified):,} å­—ç¬¦")
        print(f"å‹ç¼©æ¯”ä¾‹: {(1 - len(simplified)/len(html_content)):.1%}")
        print("\nç®€åŒ–ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")

        print("\nç®€åŒ–å‰åå¯¹æ¯”:")
        print("åŸå§‹HTMLç‰‡æ®µ:")
        print(html_content[:200] + "..." if len(html_content) > 200 else html_content)
        print("\nç®€åŒ–HTMLç‰‡æ®µ:")
        print(simplified[:200] + "..." if len(simplified) > 200 else simplified)


def demo_content_search():
    """æ¼”ç¤ºå†…å®¹æœç´¢åŠŸèƒ½"""
    print("\nğŸ” å†…å®¹æœç´¢åŠŸèƒ½å•ç‹¬æ¼”ç¤º")
    print("=" * 40)

    from tools.html_content_search import HTMLContentSearch

    searcher = HTMLContentSearch()

    # ä½¿ç”¨HTMLæ–‡ä»¶è¿›è¡Œæœç´¢æ¼”ç¤º
    sample_file = Path('examples/13_detail.html')
    if sample_file.exists():
        html_content = load_html_file(sample_file)

        # æ„å»ºæœç´¢ç´¢å¼•
        search_data = searcher.build_search_index(html_content)
        print(f"æ„å»ºæœç´¢ç´¢å¼•å®Œæˆï¼Œæ‰¾åˆ° {len(search_data.get('search_index', {}))} ä¸ªå…ƒç´ ")

        # æœç´¢ç¤ºä¾‹
        search_terms = ['ç™½æ²™', 'ä»·æ ¼', 'button', 'div']

        for term in search_terms:
            results = searcher.search_by_keyword(term)
            print(f"\næœç´¢ '{term}': æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            for i, result in enumerate(results[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
                print(f"  ç»“æœ {i+1}: {result.get('tag')} - {result.get('text_content', '')[:60]}...")

        # CSSé€‰æ‹©å™¨æœç´¢
        css_results = searcher.search_by_selector('.column_tit', 'css')
        print(f"\nCSSé€‰æ‹©å™¨ '.column_tit': æ‰¾åˆ° {len(css_results)} ä¸ªç»“æœ")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # ä¸»è¦æ¼”ç¤º
        demo_with_real_html()

        # å•ç‹¬åŠŸèƒ½æ¼”ç¤º
        demo_html_simplification()
        demo_content_search()

    except ImportError as e:
        print(f"å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…")
    except Exception as e:
        print(f"è¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
